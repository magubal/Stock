#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„° ìë™ ìˆ˜ì§‘ê¸°
- ë§¤ì¼ ìµœì‹  ë¸”ë¡œê·¸ ê¸€ì„ ìˆ˜ì§‘í•˜ì—¬ ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì €ì¥
- ì¼ìë³„/ë¸”ë¡œê±°_ìˆœë²ˆ êµ¬ì¡°ë¡œ ë°ì´í„° ì €ì¥
- ì¤‘ë³µ ì²´í¬ ë° ìë™ ì¦ë¶„ ì €ì¥ ê¸°ëŠ¥
"""

import os
import sys
import json
import hashlib
import requests

sys.stdout.reconfigure(encoding="utf-8")
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime
from urllib.parse import urlparse
from pathlib import Path
import re
from typing import List, Dict, Set

# ë³¸ë¬¸ ìº¡ì²˜ ëª¨ë“ˆ import (ì„¸ì…˜ ì¬ì‚¬ìš©)
from final_body_capture import BlogCaptureSession


def is_within_days(pub_date_str: str, days: int):
    """pub_dateê°€ ìµœê·¼ Nì¼ ì´ë‚´ì¸ì§€ í™•ì¸.

    Returns:
        (result, reason)
        - (True, "within") : Nì¼ ì´ë‚´
        - (True, "no_date") : pub_date ì—†ìŒ â†’ ìˆ˜ì§‘ í—ˆìš©
        - (True, "parse_fail") : íŒŒì‹± ì‹¤íŒ¨ â†’ ìˆ˜ì§‘ í—ˆìš©
        - (False, "too_old") : Nì¼ ì´ˆê³¼ â†’ skip
    """
    if not pub_date_str or not pub_date_str.strip():
        return True, "no_date"
    try:
        pub_dt = parsedate_to_datetime(pub_date_str.strip())
        now = datetime.now(timezone.utc)
        cutoff = now - timedelta(days=days)
        if pub_dt >= cutoff:
            return True, "within"
        else:
            return False, "too_old"
    except Exception:
        return True, "parse_fail"


class NaverBlogCollector:
    def __init__(self, base_dir: str = None):
        if base_dir is None:
            base_dir = Path(__file__).parent.parent / "data" / "naver_blog_data"
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        # ì¸ë±ìŠ¤ í´ë”
        self.index_dir = self.base_dir / "index"
        self.index_dir.mkdir(exist_ok=True)
        
        # RSS ëª©ë¡ íŒŒì¼
        self.rss_list_file = Path(__file__).parent.parent / "data" / "naver_blog_data" / "naver_bloger_rss_list.txt"
        
        # ì €ì¥ëœ ê²Œì‹œë¬¼ ì¶”ì  íŒŒì¼
        self.tracked_posts_file = self.index_dir / "tracked_posts.json"
        self.tracked_posts = self._load_tracked_posts()
        
        # ì¼ìë³„ ë¸”ë¡œê±° ì¹´ìš´í„°
        self.daily_counter_file = self.index_dir / "daily_counter.json"
        self.daily_counters = self._load_daily_counters()
        
    def _load_tracked_posts(self) -> Set[str]:
        """ì´ë¯¸ ì €ì¥ëœ ê²Œì‹œë¬¼ ëª©ë¡ ë¡œë“œ"""
        if self.tracked_posts_file.exists():
            with open(self.tracked_posts_file, 'r', encoding='utf-8') as f:
                return set(json.load(f))
        return set()
    
    def _load_daily_counters(self) -> Dict[str, Dict[str, int]]:
        """ì¼ìë³„ ë¸”ë¡œê±° ì¹´ìš´í„° ë¡œë“œ"""
        if self.daily_counter_file.exists():
            with open(self.daily_counter_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _save_tracked_posts(self):
        """ì €ì¥ëœ ê²Œì‹œë¬¼ ëª©ë¡ ì—…ë°ì´íŠ¸"""
        with open(self.tracked_posts_file, 'w', encoding='utf-8') as f:
            json.dump(list(self.tracked_posts), f, ensure_ascii=False, indent=2)
    
    def _save_daily_counters(self):
        """ì¼ìë³„ ë¸”ë¡œê±° ì¹´ìš´í„° ì €ì¥"""
        with open(self.daily_counter_file, 'w', encoding='utf-8') as f:
            json.dump(self.daily_counters, f, ensure_ascii=False, indent=2)
    
    def _get_next_sequence(self, date_str: str, blogger_name: str) -> int:
        """ì¼ìë³„ ë¸”ë¡œê±° ë‹¤ìŒ ìˆœë²ˆ ê°€ì ¸ì˜¤ê¸°"""
        if date_str not in self.daily_counters:
            self.daily_counters[date_str] = {}
        
        if blogger_name not in self.daily_counters[date_str]:
            self.daily_counters[date_str][blogger_name] = 0
        
        self.daily_counters[date_str][blogger_name] += 1
        return self.daily_counters[date_str][blogger_name]
    
    def _load_rss_list(self) -> List[Dict[str, str]]:
        """RSS í”¼ë“œ ëª©ë¡ ë¡œë“œ"""
        rss_list = []
        if not self.rss_list_file.exists():
            print(f"RSS ëª©ë¡ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.rss_list_file}")
            return rss_list
            
        with open(self.rss_list_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    parts = line.split('#')
                    rss_url = parts[0].strip()
                    blogger_name = parts[1].strip() if len(parts) > 1 else "ë¯¸ì •"
                    rss_list.append({
                        'url': rss_url,
                        'name': blogger_name
                    })
        return rss_list
    
    def _sanitize_filename(self, filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ë¬¸ì ì œê±°"""
        return re.sub(r'[<>:"/\\|?*]', '_', filename)[:100]
    
    def _download_image(self, img_url: str, save_path: Path) -> bool:
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        try:
            response = requests.get(img_url, timeout=10)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                f.write(response.content)
            return True
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {img_url}: {e}")
            return False
    
    def _extract_images_from_content(self, content: str) -> List[str]:
        """ë¸”ë¡œê·¸ ë‚´ìš©ì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        img_pattern = r'<img[^>]+src="([^"]+)"'
        matches = re.findall(img_pattern, content)
        return [url for url in matches if url.startswith('http')]
    
    def _extract_blog_content(self, blog_url: str) -> Dict:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(blog_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # ê°„ë‹¨í•œ ë‚´ìš© ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ íŒŒì‹± í•„ìš”)
            title_match = re.search(r'<meta property="og:title" content="([^"]+)"', response.text)
            desc_match = re.search(r'<meta property="og:description" content="([^"]+)"', response.text)
            
            title = title_match.group(1) if title_match else ""
            description = desc_match.group(1) if desc_match else ""
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            images = self._extract_images_from_content(response.text)
            
            return {
                'title': title,
                'description': description,
                'content': response.text[:2000],  # ì•ë¶€ë¶„ 2000ìë§Œ ì €ì¥
                'images': images
            }
        except Exception as e:
            print(f"ë¸”ë¡œê·¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ {blog_url}: {e}")
            return {'title': '', 'description': '', 'content': '', 'images': []}
    
    def collect_blogger_posts(self, blogger_info: Dict, max_posts: int = 10, days: int = 3) -> List[Dict]:
        """ê°œë³„ ë¸”ë¡œê±°ì˜ ìµœì‹  ê²Œì‹œë¬¼ ìˆ˜ì§‘

        Args:
            blogger_info: {'url': rss_url, 'name': blogger_name}
            max_posts: RSSì—ì„œ ê°€ì ¸ì˜¬ ìµœëŒ€ ì•„ì´í…œ ìˆ˜
            days: pub_date ê¸°ì¤€ ìµœê·¼ Nì¼ ì´ë‚´ë§Œ ìˆ˜ì§‘ (0=í•„í„° ì—†ìŒ)
        """
        rss_url = blogger_info['url']
        blogger_name = blogger_info['name']
        
        try:
            # .xml í™•ì¥ì ì¶”ê°€
            if not rss_url.endswith('.xml'):
                rss_url += '.xml'
            
            response = requests.get(rss_url, timeout=10)
            response.raise_for_status()
            
            root = ET.fromstring(response.text)
            items = root.find("channel").findall("item")[:max_posts]
            
            collected_posts = []
            
            for item in items:
                title = item.findtext("title", "").strip()
                link = item.findtext("link", "").strip()
                pub_date = item.findtext("pubDate", "")
                
                if not link:
                    continue

                # pub_date ë‚ ì§œ í•„í„° (days=0ì´ë©´ ë¹„í™œì„±)
                if days > 0:
                    within, reason = is_within_days(pub_date, days)
                    if not within:
                        print(f"    [SKIP-DATE] {title[:40]} (pub: {pub_date.strip()}, reason: {reason})")
                        continue
                    if reason in ("no_date", "parse_fail"):
                        print(f"    [WARN] pubDate {reason}: {title[:40]}")

                # ê²Œì‹œë¬¼ ID ìƒì„± (URL ê¸°ì¤€ - Stable Hash)
                post_id = hashlib.md5(link.encode('utf-8')).hexdigest()
                
                # ì´ë¯¸ ì €ì¥ëœ ê²Œì‹œë¬¼ì¸ì§€ í™•ì¸
                if str(post_id) in self.tracked_posts:
                    continue
                
                # ë¸”ë¡œê·¸ ë‚´ìš© ì¶”ì¶œ
                blog_content = self._extract_blog_content(link)
                
                post_data = {
                    'id': str(post_id),
                    'blogger': blogger_name,
                    'title': title,
                    'link': link,
                    'pub_date': pub_date,
                    'collected_date': datetime.now().isoformat(),
                    'content': blog_content['title'] + '\n\n' + blog_content['description'],
                    'images': blog_content['images']
                }
                
                collected_posts.append(post_data)
                self.tracked_posts.add(str(post_id))
            
            return collected_posts
            
        except Exception as e:
            print(f"RSS ìˆ˜ì§‘ ì‹¤íŒ¨ {blogger_name} ({rss_url}): {e}")
            return []
    
    def save_post(self, post_data: Dict, capture_session):
        """ê²Œì‹œë¬¼ ì €ì¥ (ë¸Œë¼ìš°ì € ì„¸ì…˜ ì¬ì‚¬ìš©)"""
        blogger_name = post_data['blogger']
        blog_link = post_data['link']
        
        # ìˆ˜ì§‘ì¼ì ê¸°ì¤€ í´ë” êµ¬ì¡°
        collected_date = datetime.fromisoformat(post_data['collected_date'])
        date_str = collected_date.strftime('%Y-%m-%d')
        
        # ë³¸ë¬¸ ì´ë¯¸ì§€ ìº¡ì²˜ (ì„¸ì…˜ ì¬ì‚¬ìš©)
        print(f"[ìº¡ì²˜] {blogger_name}: {post_data['title'][:30]}...")
        capture_result = capture_session.capture(blog_link, blogger_name)
        
        if capture_result['success']:
            image_path = Path(capture_result['file_path'])
            metadata_path = image_path.with_suffix('.json')
            
            metadata = {
                'blogger': blogger_name,
                'title': post_data['title'],
                'link': blog_link,
                'pub_date': post_data['pub_date'],
                'collected_date': post_data['collected_date'],
                'image_file': image_path.name,
                'file_size_mb': capture_result['file_size_mb']
            }
            
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            self._update_daily_summary(date_str, image_path.stem, metadata)
            print(f"  â†’ {image_path.name} ({capture_result['file_size_mb']}MB)")
        else:
            print(f"  â†’ ì‹¤íŒ¨: {capture_result['message']}")
    
    def collect_all(self, max_posts_per_blogger: int = 10, days: int = 3):
        """ëª¨ë“  ë¸”ë¡œê±°ì˜ ë°ì´í„° ìˆ˜ì§‘ (ë¸Œë¼ìš°ì € ì¬ì‚¬ìš©)

        Args:
            max_posts_per_blogger: ë¸”ë¡œê±°ë‹¹ RSS ìµœëŒ€ ì•„ì´í…œ ìˆ˜
            days: pub_date ê¸°ì¤€ ìµœê·¼ Nì¼ ì´ë‚´ë§Œ ìˆ˜ì§‘ (0=í•„í„° ì—†ìŒ)
        """
        rss_list = self._load_rss_list()
        
        if not rss_list:
            print("RSS ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return
        
        total_collected = 0
        all_posts = []
        
        # 1ë‹¨ê³„: ëª¨ë“  ë¸”ë¡œê±°ì˜ ê¸€ ëª©ë¡ ìˆ˜ì§‘ (ë¹ ë¦„)
        for blogger_info in rss_list:
            print(f"\n[RSS] {blogger_info['name']} ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
            posts = self.collect_blogger_posts(blogger_info, max_posts_per_blogger, days=days)
            all_posts.extend(posts)
        
        if not all_posts:
            print("\nìƒˆë¡œìš´ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nì´ {len(all_posts)}ê°œ ê¸€ ìº¡ì²˜ ì‹œì‘...")
        
        # 2ë‹¨ê³„: ë¸Œë¼ìš°ì € í•œ ë²ˆ ì—´ê³  ëª¨ë“  ê¸€ ìº¡ì²˜ (íš¨ìœ¨ì )
        try:
            with BlogCaptureSession() as session:
                for post in all_posts:
                    self.save_post(post, session)
                    total_collected += 1
        except Exception as e:
            print(f"\nâš ï¸ ìº¡ì²˜ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            # ì¶”ì  ê²Œì‹œë¬¼ ëª©ë¡ ë° ì¹´ìš´í„° ì—…ë°ì´íŠ¸ (í•­ìƒ ì €ì¥)
            self._save_tracked_posts()
            self._save_daily_counters()
        
        print(f"\nâœ… ì´ {total_collected}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.base_dir}")
    
    def _update_daily_summary(self, date_str: str, post_folder: str, metadata: Dict):
        """ì¼ìë³„ ìš”ì•½ ì—…ë°ì´íŠ¸"""
        summary_file = self.index_dir / "daily_summary.json"
        
        # ê¸°ì¡´ ìš”ì•½ ë¡œë“œ
        if summary_file.exists():
            with open(summary_file, 'r', encoding='utf-8') as f:
                summary = json.load(f)
        else:
            summary = {}
        
        # ë‚ ì§œë³„ ìš”ì•½ ì—…ë°ì´íŠ¸
        if date_str not in summary:
            summary[date_str] = {
                'date': date_str,
                'total_posts': 0,
                'bloggers': {},
                'posts': []
            }
        
        summary[date_str]['total_posts'] += 1
        summary[date_str]['posts'].append({
            'folder': post_folder,
            'blogger': metadata['blogger'],
            'title': metadata['title'],
            'link': metadata['link'],
            'collected_time': metadata['collected_date']
        })
        
        # ë¸”ë¡œê±°ë³„ ì¹´ìš´íŠ¸
        blogger = metadata['blogger']
        if blogger not in summary[date_str]['bloggers']:
            summary[date_str]['bloggers'][blogger] = 0
        summary[date_str]['bloggers'][blogger] += 1
        
        # ìš”ì•½ ì €ì¥
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
    
    def get_recent_posts(self, days: int = 3) -> Dict:
        """ìµœê·¼ ì¼ìë³„ ê²Œì‹œë¬¼ ìš”ì•½ ê°€ì ¸ì˜¤ê¸°"""
        summary_file = self.index_dir / "daily_summary.json"
        
        if not summary_file.exists():
            return {}
        
        with open(summary_file, 'r', encoding='utf-8') as f:
            summary = json.load(f)
        
        # ìµœê·¼ ë©°ì¹ ì¹˜ë§Œ í•„í„°ë§
        recent_dates = []
        today = datetime.now().date()
        
        for i in range(days):
            date = today - timedelta(days=i)
            date_str = date.strftime('%Y-%m-%d')
            if date_str in summary:
                recent_dates.append(date_str)
        
        return {date: summary[date] for date in sorted(recent_dates, reverse=True)}

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    parser = argparse.ArgumentParser(description="ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘ê¸°")
    parser.add_argument("--days", type=int, default=3,
                        help="ìµœê·¼ Nì¼ ì´ë‚´ ë°œí–‰ ê¸€ë§Œ ìˆ˜ì§‘ (0=í•„í„°ì—†ìŒ, ê¸°ë³¸=3)")
    parser.add_argument("--max-posts", type=int, default=10,
                        help="ë¸”ë¡œê±°ë‹¹ RSS ìµœëŒ€ ì•„ì´í…œ ìˆ˜ (ê¸°ë³¸=10)")
    args = parser.parse_args()

    collector = NaverBlogCollector()
    collector.collect_all(max_posts_per_blogger=args.max_posts, days=args.days)

if __name__ == "__main__":
    main()
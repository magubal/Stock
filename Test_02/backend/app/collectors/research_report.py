from typing import List, Dict, Any
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import re
from datetime import datetime
from urllib.parse import urljoin, urlparse
from .base import BaseCollector

class ResearchReportCollector(BaseCollector):
    """ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        self.base_urls = {
            'kiwoom': 'https://www.kiwoom.com',
            'miraeasset': 'https://www.miraeasset.com',
            'kbsec': 'https://securities.kbfg.com',
            'nhqv': 'https://www.nhqv.com'
        }
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
    
    async def collect(self) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"""
        reports = []
        
        # ì—¬ëŸ¬ ì¦ê¶Œì‚¬ì—ì„œ ë³‘ë ¬ë¡œ ìˆ˜ì§‘
        tasks = [
            self.collect_kiwoom_reports(),
            self.collect_miraeasset_reports(),
            self.collect_kbsec_reports(),
            self.collect_nhqv_reports()
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, list):
                reports.extend(result)
            else:
                print(f"âš ï¸ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì˜¤ë¥˜: {result}")
        
        return reports
    
    async def collect_kiwoom_reports(self) -> List[Dict[str, Any]]:
        """í‚¤ì›€ì¦ê¶Œ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"""
        url = "https://www.kiwoom.com/h/invest/research/report/recommend.jspx"
        reports = []
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            try:
                async with session.get(url) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ë¦¬í¬íŠ¸ ëª©ë¡ íŒŒì‹±
                        report_items = soup.select('.report-list li')
                        
                        for item in report_items:
                            try:
                                title_elem = item.select_one('.tit')
                                date_elem = item.select_one('.date')
                                link_elem = item.select_one('a')
                                
                                if title_elem and date_elem:
                                    title = title_elem.get_text(strip=True)
                                    date_str = date_elem.get_text(strip=True)
                                    link = urljoin(self.base_urls['kiwoom'], link_elem['href']) if link_elem else None
                                    
                                    reports.append({
                                        'title': title,
                                        'date': date_str,
                                        'link': link,
                                        'brokerage': 'í‚¤ì›€ì¦ê¶Œ',
                                        'source': 'kiwoom'
                                    })
                            except Exception as e:
                                print(f"í‚¤ì›€ì¦ê¶Œ ë¦¬í¬íŠ¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
                                continue
                        
            except Exception as e:
                print(f"í‚¤ì›€ì¦ê¶Œ ì ‘ì† ì˜¤ë¥˜: {e}")
        
        return reports
    
    async def collect_miraeasset_reports(self) -> List[Dict[str, Any]]:
        """ë¯¸ë˜ì—ì…‹ì¦ê¶Œ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"""
        url = "https://www.miraeasset.com/contents/research/researchList.jsp"
        reports = []
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            try:
                async with session.get(url) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ë¦¬í¬íŠ¸ ëª©ë¡ íŒŒì‹±
                        report_items = soup.select('.research-list tr')
                        
                        for item in report_items:
                            try:
                                title_elem = item.select_one('.title')
                                date_elem = item.select_one('.date')
                                link_elem = item.select_one('a')
                                
                                if title_elem and date_elem:
                                    title = title_elem.get_text(strip=True)
                                    date_str = date_elem.get_text(strip=True)
                                    link = urljoin(self.base_urls['miraeasset'], link_elem['href']) if link_elem else None
                                    
                                    reports.append({
                                        'title': title,
                                        'date': date_str,
                                        'link': link,
                                        'brokerage': 'ë¯¸ë˜ì—ì…‹ì¦ê¶Œ',
                                        'source': 'miraeasset'
                                    })
                            except Exception as e:
                                print(f"ë¯¸ë˜ì—ì…‹ì¦ê¶Œ ë¦¬í¬íŠ¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
                                continue
                        
            except Exception as e:
                print(f"ë¯¸ë˜ì—ì…‹ì¦ê¶Œ ì ‘ì† ì˜¤ë¥˜: {e}")
        
        return reports
    
    async def collect_kbsec_reports(self) -> List[Dict[str, Any]]:
        """KBì¦ê¶Œ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"""
        url = "https://securities.kbfg.com/research/report/reportList.do"
        reports = []
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            try:
                async with session.get(url) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ë¦¬í¬íŠ¸ ëª©ë¡ íŒŒì‹±
                        report_items = soup.select('.report-row')
                        
                        for item in report_items:
                            try:
                                title_elem = item.select_one('.report-title')
                                date_elem = item.select_one('.report-date')
                                link_elem = item.select_one('a')
                                
                                if title_elem and date_elem:
                                    title = title_elem.get_text(strip=True)
                                    date_str = date_elem.get_text(strip=True)
                                    link = urljoin(self.base_urls['kbsec'], link_elem['href']) if link_elem else None
                                    
                                    reports.append({
                                        'title': title,
                                        'date': date_str,
                                        'link': link,
                                        'brokerage': 'KBì¦ê¶Œ',
                                        'source': 'kbsec'
                                    })
                            except Exception as e:
                                print(f"KBì¦ê¶Œ ë¦¬í¬íŠ¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
                                continue
                        
            except Exception as e:
                print(f"KBì¦ê¶Œ ì ‘ì† ì˜¤ë¥˜: {e}")
        
        return reports
    
    async def collect_nhqv_reports(self) -> List[Dict[str, Any]]:
        """NHíˆ¬ìì¦ê¶Œ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"""
        url = "https://www.nhqv.com/research/researchList.do"
        reports = []
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            try:
                async with session.get(url) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ë¦¬í¬íŠ¸ ëª©ë¡ íŒŒì‹±
                        report_items = soup.select('.research-item')
                        
                        for item in report_items:
                            try:
                                title_elem = item.select_one('.item-title')
                                date_elem = item.select_one('.item-date')
                                link_elem = item.select_one('a')
                                
                                if title_elem and date_elem:
                                    title = title_elem.get_text(strip=True)
                                    date_str = date_elem.get_text(strip=True)
                                    link = urljoin(self.base_urls['nhqv'], link_elem['href']) if link_elem else None
                                    
                                    reports.append({
                                        'title': title,
                                        'date': date_str,
                                        'link': link,
                                        'brokerage': 'NHíˆ¬ìì¦ê¶Œ',
                                        'source': 'nhqv'
                                    })
                            except Exception as e:
                                print(f"NHíˆ¬ìì¦ê¶Œ ë¦¬í¬íŠ¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
                                continue
                        
            except Exception as e:
                print(f"NHíˆ¬ìì¦ê¶Œ ì ‘ì† ì˜¤ë¥˜: {e}")
        
        return reports
    
    def parse_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë¦¬í¬íŠ¸ ë°ì´í„° íŒŒì‹± ë° í‘œì¤€í™”"""
        
        # ì¢…ëª© ì½”ë“œ ì¶”ì¶œ (ì •ê·œì‹)
        title = raw_data.get('title', '')
        stock_code = self.extract_stock_code(title)
        stock_name = self.extract_stock_name(title)
        
        # ì¶”ì²œ ì˜ê²¬ ì¶”ì¶œ
        recommendation = self.extract_recommendation(title)
        
        # ëª©í‘œê°€ ì¶”ì¶œ
        target_price = self.extract_target_price(title)
        
        # ë‚ ì§œ íŒŒì‹±
        parsed_date = self.parse_date(raw_data.get('date', ''))
        
        return {
            'title': title,
            'content': '',  # ìƒì„¸ ë‚´ìš©ì€ ë³„ë„ ìˆ˜ì§‘ í•„ìš”
            'pdf_url': raw_data.get('link'),
            'brokerage': raw_data.get('brokerage'),
            'author': '',  # ì‘ì„±ì ì •ë³´ëŠ” ìƒì„¸ í˜ì´ì§€ì—ì„œ ìˆ˜ì§‘ í•„ìš”
            'target_price': target_price,
            'recommendation': recommendation,
            'published_at': parsed_date,
            'stock_code': stock_code,
            'stock_name': stock_name,
            'source': raw_data.get('source'),
            'raw_data': raw_data
        }
    
    def extract_stock_code(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ì¢…ëª© ì½”ë“œ ì¶”ì¶œ"""
        # ì¢…ëª© ì½”ë“œ íŒ¨í„´: A005930, 005930 ë“±
        pattern = r'[A]?\d{6}'
        match = re.search(pattern, text)
        return match.group() if match else ''
    
    def extract_stock_name(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ì¢…ëª©ëª… ì¶”ì¶œ (ë‹¨ìˆœ ë²„ì „)"""
        # ì¢…ëª©ëª… ì¶”ì¶œ ë¡œì§ (ê°œì„  í•„ìš”)
        stock_names = ['ì‚¼ì„±ì „ì', 'LGì—ë„ˆì§€ì†”ë£¨ì…˜', 'SKí•˜ì´ë‹‰ìŠ¤', 'í˜„ëŒ€ì°¨', 'ê¸°ì•„']
        for name in stock_names:
            if name in text:
                return name
        return ''
    
    def extract_recommendation(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì²œ ì˜ê²¬ ì¶”ì¶œ"""
        text_lower = text.lower()
        
        if 'ë§¤ìˆ˜' in text or 'buy' in text_lower:
            return 'buy'
        elif 'ë§¤ë„' in text or 'sell' in text_lower:
            return 'sell'
        elif 'ë³´ìœ ' in text or 'hold' in text_lower or 'neutral' in text_lower:
            return 'hold'
        
        return 'hold'  # ê¸°ë³¸ê°’
    
    def extract_target_price(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ì—ì„œ ëª©í‘œê°€ ì¶”ì¶œ"""
        # ëª©í‘œê°€ íŒ¨í„´: 80,000ì›, 80000ì› ë“±
        pattern = r'[\d,]+ì›'
        matches = re.findall(pattern, text)
        
        if matches:
            # ì²«ë²ˆì§¸ ê°€ê²©ì„ ëª©í‘œê°€ë¡œ ê°„ì£¼
            price_str = matches[0].replace(',', '').replace('ì›', '')
            try:
                return float(price_str)
            except ValueError:
                pass
        
        return None
    
    def parse_date(self, date_str: str) -> datetime:
        """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì§€ì›
        date_formats = [
            '%Y-%m-%d',
            '%Y.%m.%d',
            '%Y/%m/%d',
            '%m-%d',
            '%m.%d',
            '%m/%d'
        ]
        
        for fmt in date_formats:
            try:
                if len(date_str.split('.')[0]) == 4:  # ì—°ë„ í¬í•¨
                    return datetime.strptime(date_str, fmt)
                else:  # ì—°ë„ ì—†ìœ¼ë©´ í˜„ì¬ ì—°ë„ ì¶”ê°€
                    current_year = datetime.now().year
                    date_with_year = f"{current_year}.{date_str}"
                    return datetime.strptime(date_with_year, f"%Y.{fmt}")
            except ValueError:
                continue
        
        # íŒŒì‹± ì‹¤íŒ¨ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
        return datetime.now()
    
    async def save_to_db(self, data: Dict[str, Any]):
        """ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (êµ¬í˜„ ì˜ˆì •)"""
        # TODO: SQLAlchemyë¥¼ ì‚¬ìš©í•œ DB ì €ì¥ ë¡œì§ êµ¬í˜„
        print(f"ğŸ’¾ DB ì €ì¥ ì˜ˆì •: {data['title'][:50]}...")
        pass
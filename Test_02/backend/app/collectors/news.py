from typing import List, Dict, Any
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse
from .base import BaseCollector

class NewsCollector(BaseCollector):
    """ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° - ì£¼ìš” ê²½ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸"""
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        self.news_sources = {
            'yna': {
                'name': 'ì—°í•©ë‰´ìŠ¤',
                'base_url': 'https://www.yna.co.kr',
                'list_url': 'https://www.yna.co.kr/economy',
                'selectors': {
                    'list': '.news-list li',
                    'title': '.tit a',
                    'time': '.time',
                    'content': '.story-news article'
                }
            },
            'hankyung': {
                'name': 'í•œêµ­ê²½ì œ',
                'base_url': 'https://www.hankyung.com',
                'list_url': 'https://www.hankyung.com/economy',
                'selectors': {
                    'list': '.article-list li',
                    'title': '.tit a',
                    'time': '.date',
                    'content': '.article-view'
                }
            },
            'maeil': {
                'name': 'ë§¤ì¼ê²½ì œ',
                'base_url': 'https://www.mk.co.kr',
                'list_url': 'https://www.mk.co.kr/news/economy/',
                'selectors': {
                    'list': '.article_list li',
                    'title': '.tit a',
                    'time': '.date',
                    'content': '.center_col'
                }
            },
            'edaily': {
                'name': 'ì´ë°ì¼ë¦¬',
                'base_url': 'https://www.edaily.co.kr',
                'list_url': 'https://www.edaily.co.kr/News/NewsRead.ec?newsId=026',
                'selectors': {
                    'list': '.news_list li',
                    'title': '.news_title a',
                    'time': '.date_time',
                    'content': '.article_body'
                }
            }
        }
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
    
    async def collect(self) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_news = []
        
        # ì—¬ëŸ¬ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ë¡œ ìˆ˜ì§‘
        tasks = [
            self.collect_yna_news(),
            self.collect_hankyung_news(),
            self.collect_maeil_news(),
            self.collect_edaily_news()
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, list):
                all_news.extend(result)
            else:
                print(f"âš ï¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {result}")
        
        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        seen_titles = set()
        unique_news = []
        for news in all_news:
            title = news.get('title', '')
            if title not in seen_titles:
                seen_titles.add(title)
                unique_news.append(news)
        
        # ìµœì‹  ë‰´ìŠ¤ 100ê°œë¡œ ì œí•œ
        return sorted(unique_news, key=lambda x: x.get('published_at', ''), reverse=True)[:100]
    
    async def collect_yna_news(self) -> List[Dict[str, Any]]:
        """ì—°í•©ë‰´ìŠ¤ ìˆ˜ì§‘"""
        source_config = self.news_sources['yna']
        news_list = []
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            try:
                # ë‰´ìŠ¤ ëª©ë¡ í˜ì´ì§€
                async with session.get(source_config['list_url']) as response:
                    if response.status != 200:
                        print(f"ì—°í•©ë‰´ìŠ¤ ì ‘ì† ì‹¤íŒ¨: {response.status}")
                        return []
                    
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ íŒŒì‹±
                    news_items = soup.select(source_config['selectors']['list'])
                    
                    for item in news_items[:20]:  # ìµœëŒ€ 20ê°œë§Œ ìˆ˜ì§‘
                        try:
                            title_elem = item.select_one(source_config['selectors']['title'])
                            time_elem = item.select_one(source_config['selectors']['time'])
                            
                            if not title_elem:
                                continue
                            
                            title = title_elem.get_text(strip=True)
                            link = title_elem.get('href', '')
                            full_url = urljoin(source_config['base_url'], link)
                            published_time = self.parse_news_time(time_elem.get_text(strip=True)) if time_elem else datetime.now()
                            
                            # ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘
                            content = await self.get_news_content(session, full_url, source_config['selectors']['content'])
                            
                            news_data = {
                                'title': title,
                                'content': content,
                                'url': full_url,
                                'published_at': published_time,
                                'source': 'yna',
                                'source_name': source_config['name']
                            }
                            
                            news_list.append(news_data)
                            
                        except Exception as e:
                            print(f"ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
                            continue
                    
            except Exception as e:
                print(f"ì—°í•©ë‰´ìŠ¤ ì „ì²´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        return news_list
    
    async def collect_hankyung_news(self) -> List[Dict[str, Any]]:
        """í•œêµ­ê²½ì œ ìˆ˜ì§‘"""
        source_config = self.news_sources['hankyung']
        news_list = []
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            try:
                async with session.get(source_config['list_url']) as response:
                    if response.status != 200:
                        print(f"í•œêµ­ê²½ì œ ì ‘ì† ì‹¤íŒ¨: {response.status}")
                        return []
                    
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    news_items = soup.select(source_config['selectors']['list'])
                    
                    for item in news_items[:20]:
                        try:
                            title_elem = item.select_one(source_config['selectors']['title'])
                            time_elem = item.select_one(source_config['selectors']['time'])
                            
                            if not title_elem:
                                continue
                            
                            title = title_elem.get_text(strip=True)
                            link = title_elem.get('href', '')
                            full_url = urljoin(source_config['base_url'], link)
                            published_time = self.parse_news_time(time_elem.get_text(strip=True)) if time_elem else datetime.now()
                            
                            content = await self.get_news_content(session, full_url, source_config['selectors']['content'])
                            
                            news_list.append({
                                'title': title,
                                'content': content,
                                'url': full_url,
                                'published_at': published_time,
                                'source': 'hankyung',
                                'source_name': source_config['name']
                            })
                            
                        except Exception as e:
                            print(f"í•œêµ­ê²½ì œ ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
                            continue
                    
            except Exception as e:
                print(f"í•œêµ­ê²½ì œ ì „ì²´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        return news_list
    
    async def collect_maeil_news(self) -> List[Dict[str, Any]]:
        """ë§¤ì¼ê²½ì œ ìˆ˜ì§‘"""
        source_config = self.news_sources['maeil']
        news_list = []
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            try:
                async with session.get(source_config['list_url']) as response:
                    if response.status != 200:
                        print(f"ë§¤ì¼ê²½ì œ ì ‘ì† ì‹¤íŒ¨: {response.status}")
                        return []
                    
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    news_items = soup.select(source_config['selectors']['list'])
                    
                    for item in news_items[:20]:
                        try:
                            title_elem = item.select_one(source_config['selectors']['title'])
                            time_elem = item.select_one(source_config['selectors']['time'])
                            
                            if not title_elem:
                                continue
                            
                            title = title_elem.get_text(strip=True)
                            link = title_elem.get('href', '')
                            full_url = urljoin(source_config['base_url'], link)
                            published_time = self.parse_news_time(time_elem.get_text(strip=True)) if time_elem else datetime.now()
                            
                            content = await self.get_news_content(session, full_url, source_config['selectors']['content'])
                            
                            news_list.append({
                                'title': title,
                                'content': content,
                                'url': full_url,
                                'published_at': published_time,
                                'source': 'maeil',
                                'source_name': source_config['name']
                            })
                            
                        except Exception as e:
                            print(f"ë§¤ì¼ê²½ì œ ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
                            continue
                    
            except Exception as e:
                print(f"ë§¤ì¼ê²½ì œ ì „ì²´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        return news_list
    
    async def collect_edaily_news(self) -> List[Dict[str, Any]]:
        """ì´ë°ì¼ë¦¬ ìˆ˜ì§‘"""
        source_config = self.news_sources['edaily']
        news_list = []
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            try:
                async with session.get(source_config['list_url']) as response:
                    if response.status != 200:
                        print(f"ì´ë°ì¼ë¦¬ ì ‘ì† ì‹¤íŒ¨: {response.status}")
                        return []
                    
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    news_items = soup.select(source_config['selectors']['list'])
                    
                    for item in news_items[:20]:
                        try:
                            title_elem = item.select_one(source_config['selectors']['title'])
                            time_elem = item.select_one(source_config['selectors']['time'])
                            
                            if not title_elem:
                                continue
                            
                            title = title_elem.get_text(strip=True)
                            link = title_elem.get('href', '')
                            full_url = urljoin(source_config['base_url'], link)
                            published_time = self.parse_news_time(time_elem.get_text(strip=True)) if time_elem else datetime.now()
                            
                            content = await self.get_news_content(session, full_url, source_config['selectors']['content'])
                            
                            news_list.append({
                                'title': title,
                                'content': content,
                                'url': full_url,
                                'published_at': published_time,
                                'source': 'edaily',
                                'source_name': source_config['name']
                            })
                            
                        except Exception as e:
                            print(f"ì´ë°ì¼ë¦¬ ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
                            continue
                    
            except Exception as e:
                print(f"ì´ë°ì¼ë¦¬ ì „ì²´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        return news_list
    
    async def get_news_content(self, session: aiohttp.ClientSession, url: str, content_selector: str) -> str:
        """ê°œë³„ ë‰´ìŠ¤ ê¸°ì‚¬ ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘"""
        try:
            # ë”œë ˆì´ ì ìš© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            await asyncio.sleep(self.config.get('request_delay', 1.0))
            
            async with session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ê¸°ì‚¬ ë‚´ìš© íŒŒì‹±
                    content_elem = soup.select_one(content_selector)
                    if content_elem:
                        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                        for tag in content_elem.find_all(['script', 'style', 'nav', 'footer', 'aside']):
                            tag.decompose()
                        
                        content = content_elem.get_text(strip=True)
                        # ì—¬ëŸ¬ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì •ë¦¬
                        content = re.sub(r'\s+', ' ', content)
                        return content
                    else:
                        # ëŒ€ì²´ ì„ íƒì ì‹œë„
                        alt_selectors = [
                            '.article-body',
                            '#articleBody',
                            '.news-body',
                            '.content',
                            'article p'
                        ]
                        
                        for selector in alt_selectors:
                            content_elem = soup.select_one(selector)
                            if content_elem:
                                return content_elem.get_text(strip=True)
                        
                        return "ë‚´ìš© ìˆ˜ì§‘ ì‹¤íŒ¨"
                else:
                    return f"HTTP ì˜¤ë¥˜: {response.status}"
                    
        except Exception as e:
            return f"ë‚´ìš© ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}"
    
    def parse_news_time(self, time_str: str) -> datetime:
        """ë‰´ìŠ¤ ì‹œê°„ íŒŒì‹±"""
        if not time_str:
            return datetime.now()
        
        # ë‹¤ì–‘í•œ ì‹œê°„ í˜•ì‹ íŒŒì‹±
        time_patterns = [
            r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})\s*(\d{1,2}):(\d{2})',  # 2023-12-01 14:30
            r'(\d{1,2})[-./](\d{1,2})\s*(\d{1,2}):(\d{2})',  # 12-01 14:30
            r'(\d{1,2})ì›”\s*(\d{1,2})ì¼\s*(\d{1,2}):(\d{2})',  # 12ì›” 1ì¼ 14:30
            r'(\d{1,2}):(\d{2})',  # 14:30 (ì˜¤ëŠ˜ ë‰´ìŠ¤)
        ]
        
        for pattern in time_patterns:
            match = re.search(pattern, time_str)
            if match:
                groups = match.groups()
                
                if len(groups) == 5:  # ë…„ì›”ì¼ì‹œë¶„
                    year, month, day, hour, minute = map(int, groups)
                elif len(groups) == 4:  # ì›”ì¼ì‹œë¶„
                    now = datetime.now()
                    year = now.year
                    month, day, hour, minute = map(int, groups)
                elif len(groups) == 2:  # ì‹œë¶„
                    now = datetime.now()
                    year, month, day = now.year, now.month, now.day
                    hour, minute = map(int, groups)
                else:
                    continue
                
                return datetime(year, month, day, hour, minute)
        
        # íŒŒì‹± ì‹¤íŒ¨ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
        return datetime.now()
    
    def parse_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ë°ì´í„° íŒŒì‹± ë° í‘œì¤€í™”"""
        
        # ì¢…ëª© ì½”ë“œ ì¶”ì¶œ
        title = raw_data.get('title', '')
        content = raw_data.get('content', '')
        full_text = f"{title} {content}"
        
        stock_codes = self.extract_stock_codes(full_text)
        stock_names = self.extract_stock_names(full_text)
        
        # ì¤‘ìš”ë„ í‰ê°€
        importance_score = self.calculate_importance(title, content)
        
        # ê°ì„± ë¶„ì„ (ê°„ë‹¨ ë²„ì „)
        sentiment_score = self.analyze_sentiment(title, content)
        
        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        category = self.classify_news_category(title, content)
        
        return {
            'title': raw_data.get('title'),
            'content': raw_data.get('content'),
            'url': raw_data.get('url'),
            'published_at': raw_data.get('published_at'),
            'author': raw_data.get('author', ''),
            'source_id': None,  # TODO: ë°ì´í„°ë² ì´ìŠ¤ ID ì„¤ì •
            'sentiment_score': sentiment_score,
            'importance_score': importance_score,
            'stock_mentions': ','.join(stock_codes) if stock_codes else None,
            'category': category,
            'source_name': raw_data.get('source_name'),
            'raw_source': raw_data.get('source')
        }
    
    def extract_stock_codes(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì¢…ëª© ì½”ë“œ ì¶”ì¶œ"""
        # ì¢…ëª© ì½”ë“œ íŒ¨í„´: A005930, 005930 ë“±
        pattern = r'[A]?\d{6}'
        codes = re.findall(pattern, text)
        return list(set(codes))  # ì¤‘ë³µ ì œê±°
    
    def extract_stock_names(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ì¢…ëª©ëª… ì¶”ì¶œ"""
        # ì£¼ìš” ì¢…ëª©ëª… ë¦¬ìŠ¤íŠ¸ (í™•ì¥ ê°€ëŠ¥)
        stock_names = [
            'ì‚¼ì„±ì „ì', 'LGì—ë„ˆì§€ì†”ë£¨ì…˜', 'SKí•˜ì´ë‹‰ìŠ¤', 'í˜„ëŒ€ì°¨', 'ê¸°ì•„',
            'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤', 'ì…€íŠ¸ë¦¬ì˜¨', 'ì‚¼ì„±SDI', 'LGí™”í•™', 'POSCOí™€ë”©ìŠ¤',
            'NAVER', 'ì¹´ì¹´ì˜¤', 'ì‚¼ì„±ì „ê¸°', 'SKí…”ë ˆì½¤', 'LGìœ í”ŒëŸ¬ìŠ¤',
            'KBê¸ˆìœµ', 'ì‹ í•œì§€ì£¼', 'í•˜ë‚˜ê¸ˆìœµì§€ì£¼', 'ìš°ë¦¬ê¸ˆìœµì§€ì£¼', 'KBê¸ˆìœµ'
        ]
        
        found_names = []
        text_lower = text.lower()
        
        for name in stock_names:
            if name in text:
                found_names.append(name)
        
        return list(set(found_names))
    
    def calculate_importance(self, title: str, content: str) -> float:
        """ë‰´ìŠ¤ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° (0-1)"""
        importance = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        # ì œëª©ì— í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨ ì‹œ ê°€ì¤‘ì¹˜
        title_lower = title.lower()
        
        high_importance_keywords = [
            'ì‹¤ì ', 'ì‹¤ì ë°œí‘œ', 'ë¶„ê¸°', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ìˆœì´ìµ',
            'ê²°ì‚°', 'ë°°ë‹¹', 'ë¶„ë°°', 'ì•¡ë©´ë¶„í• ', 'ì¦ì', 'ìœ ìƒì¦ì',
            'M&A', 'ì¸ìˆ˜', 'í•©ë³‘', 'ë§¤ê°', 'ìŠ¤í•€ì˜¤í”„',
            'ì •ì±…', 'ê·œì œ', 'ê¸ˆë¦¬', 'í™˜ìœ¨', 'ìœ ê°€', 'ê¸ˆê°’'
        ]
        
        for keyword in high_importance_keywords:
            if keyword in title_lower:
                importance += 0.1
        
        # ì¢…ëª© ì½”ë“œ ì–¸ê¸‰ ì‹œ ê°€ì¤‘ì¹˜
        if self.extract_stock_codes(f"{title} {content}"):
            importance += 0.1
        
        # ì£¼ìš” ì¢…ëª©ëª… ì–¸ê¸‰ ì‹œ ê°€ì¤‘ì¹˜
        if self.extract_stock_names(f"{title} {content}"):
            importance += 0.1
        
        # ê¸¸ì´ê°€ ê¸´ ê¸°ì‚¬ëŠ” ì¤‘ìš”í•  ê°€ëŠ¥ì„±
        if len(content) > 1000:
            importance += 0.05
        
        return min(importance, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
    
    def analyze_sentiment(self, title: str, content: str) -> float:
        """ê°„ë‹¨í•œ ê°ì„± ë¶„ì„ (-1 ë¶€ì • ~ 1 ê¸ì •)"""
        text = f"{title} {content}".lower()
        
        # ê¸ì • í‚¤ì›Œë“œ
        positive_keywords = [
            'ìƒìŠ¹', 'ê¸‰ë“±', 'ìƒí–¥', 'í˜¸ì¡°', 'í˜¸ì‹¤ì ', 'ê°œì„ ', 'ì¦ê°€',
            'ìµœê³ ', 'ì‹ ê³ ', 'ëŒíŒŒ', 'ì„±ì¥', 'ê¸°ëŒ€', 'ê¸ì •', 'ë‚™ê´€'
        ]
        
        # ë¶€ì • í‚¤ì›Œë“œ
        negative_keywords = [
            'í•˜ë½', 'ê¸‰ë½', 'í•˜í–¥', 'ë¶€ì§„', 'ì•…í™”', 'ê°ì†Œ', 'í•˜ë½ì„¸',
            'ìµœì €', 'ì‹ ì €', 'í•˜ë½', 'ìœ„ê¸°', 'ë¦¬ìŠ¤í¬', 'ìš°ë ¤', 'ë¶€ì •'
        ]
        
        positive_count = sum(1 for keyword in positive_keywords if keyword in text)
        negative_count = sum(1 for keyword in negative_keywords if keyword in text)
        
        total_keywords = positive_count + negative_count
        if total_keywords == 0:
            return 0.0  # ì¤‘ë¦½
        
        return (positive_count - negative_count) / total_keywords
    
    def classify_news_category(self, title: str, content: str) -> str:
        """ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        text = f"{title} {content}".lower()
        
        categories = {
            'ì‹œì¥ë™í–¥': ['ì½”ìŠ¤í”¼', 'ì½”ìŠ¤ë‹¥', 'ì‹œì¥', 'ì§€ìˆ˜', 'ì£¼ê°€', 'íˆ¬ì'],
            'ì‹¤ì ê³µì‹œ': ['ì‹¤ì ', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ìˆœì´ìµ', 'ë¶„ê¸°', 'ê²°ì‚°'],
            'ê¸°ì—…ê³µì‹œ': ['ê³µì‹œ', 'M&A', 'ì¸ìˆ˜', 'í•©ë³‘', 'ë§¤ê°', 'ì§€ë¶„'],
            'ê¸ˆìœµì •ì±…': ['ê¸ˆë¦¬', 'í™˜ìœ¨', 'ì •ì±…', 'ê·œì œ', 'í•œì€', 'Fed'],
            'ì‚°ì—…ë™í–¥': ['ì‚°ì—…', 'ì„¹í„°', 'ì‹œì¥', 'íŠ¸ë Œë“œ', 'ì „ë§'],
            'í•´ì™¸ì¦ì‹œ': ['ë‹¤ìš°', 'ë‚˜ìŠ¤ë‹¥', 'S&P', 'ì›”ê°€', 'ë¯¸êµ­', 'ì¤‘êµ­', 'ì¼ë³¸']
        }
        
        max_score = 0
        best_category = 'ì¼ë°˜ë‰´ìŠ¤'
        
        for category, keywords in categories.items():
            score = sum(1 for keyword in keywords if keyword in text)
            if score > max_score:
                max_score = score
                best_category = category
        
        return best_category
    
    async def save_to_db(self, data: Dict[str, Any]):
        """ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (êµ¬í˜„ ì˜ˆì •)"""
        # TODO: SQLAlchemyë¥¼ ì‚¬ìš©í•œ DB ì €ì¥ ë¡œì§ êµ¬í˜„
        print(f"ğŸ’¾ ë‰´ìŠ¤ ì €ì¥ ì˜ˆì •: {data['title'][:50]}...")
        pass